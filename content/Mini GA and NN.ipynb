{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Genetic Algorithm in 15 lines of Python code</h1>\n",
    "<h4>A simple yet powerful genetic algorithm implementation used to train a neural network in 15 lines of code.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:smaller;\"><b>Disclaimer: </b> I am not a machine learning expert by any means, I mostly do web development, so this is not my forte at all, but I have enjoyed messing around writing basic neural nets and genetic algorithms and am just trying to share what little I've learned to other neophytes out there.</p>\n",
    "<p><b>Summary:</b>\n",
    "This is a spinoff of a really great tutorial called \"A Neural Network in 11 lines of Python\" found here: < http://iamtrask.github.io/2015/07/12/basic-python-network/ > So please go through that article first otherwise this may not make any sense. <br />\n",
    "Here I will show you how I wrote a basic genetic algorithm (GA) that finds an optimal set of weights to train the neural network. I'm not going to go into detail about what a genetic algorithm is, so if you're already not familiar with them, please do some googling. \n",
    "<p style=\"font-size:small;\">By the way, GAs are generally much slower than good ol' gradient descent, but I think applying GAs to a simple neural net is a more fun way to learn it. Also, GAs may be good for finding an optimal set of <em>hyperparameters</em> for a neural net (e.g. the net architecture).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Just Give Me The Code:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random, numpy as np, NeuralNet as NN\n",
    "params = [100, 0.05, 250, 3, 20]\n",
    "curPop = np.random.choice(np.arange(-15,15,step=0.01),size=(params[0],params[3]),replace=False)\n",
    "nextPop = np.zeros((curPop.shape[0], curPop.shape[1]))\n",
    "fitVec = np.zeros((params[0], 2))\n",
    "for i in range(params[2]):\n",
    "\tfitVec = np.array([np.array([x, np.sum(NN.costFunction(NN.X, NN.y, curPop[x].reshape(3,1)))]) for x in range(params[0])])\n",
    "\twinners = np.zeros((params[4], params[3])) #20x2\n",
    "\tfor n in range(len(winners)):\n",
    "\t\tselected = np.random.choice(range(len(fitVec)), params[4]/2, replace=False)\n",
    "\t\twnr = np.argmin(fitVec[selected,1])\n",
    "\t\twinners[n] = curPop[int(fitVec[selected[wnr]][0])]\n",
    "\tnextPop[:len(winners)] = winners\n",
    "\tnextPop[len(winners):] = np.array([np.array(np.random.permutation(np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0))) for x in range(winners.shape[1])]).T\n",
    "\tcurPop = np.multiply(nextPop, np.matrix([np.float(np.random.normal(0,2,1)) if random.random() < params[1] else 1 for x in range(nextPop.size)]).reshape(nextPop.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />Ok, so I'm assuming that code is completely not helpful at this point, and in fact, it won't even run if you tried to copy and paste and run it because you also need the code for the Neural Network, which I stored in a separate file and imported. Let's first talk about the general steps in implementing a genetic algorithm and then we'll break down the code line by line, add in some print() statements to see what's going on, and maybe even make some fancy graphs.<p>Essentially, a genetic algorithm is a search algorithm that will hopefully find an optimal solution through a process that simulates natural selection and evolution. Here's the overall flow for how they work:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We generate a population of random potential solutions\n",
    "+ Then we iterate through this population and assess the fitness of (i.e. how good of a solution) each solution\n",
    "+ We prefentially select solutions with higher fitness to survive and make it to the next generation.\n",
    "Solutions with higher fitness have a higher probability of being selected\n",
    "+ These \"winner\" solutions then \"mate\" and produce offspring solutions. For example, if our solutions are simply \n",
    "vectors of integers, then mating vector1 with vector2 involves taking a few elements from vector1 and combining it with a few elements of vector2 to make a new offspring vector of the same dimensions. Vector1: [1 2 3], Vector2: [4 5 6]. Vector1 mates with Vector2 to produce [4 5 3] and [1 2 6]\n",
    "+ So now we have a new population with the top solutions from the last generation along with new offspring solutions, at this point, we will iterate over our solutions and randomly mutate some of them to make sure to introduce new \"genetic diversity\" into every generation to prevent premature convergence on a local optimum.\n",
    "+ Repeat this process for X number of generations or until we have a sufficiently good solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick review, the iamtrask article shows you how to implement a really simple 2-layer (1 input layer, 1 output layer) neural network that is trained to solve this problem:\n",
    "<table class=\"tg\" style=\"width: 234px; margin-right: 65vw;\">\n",
    "  <tbody><tr>\n",
    "    <th class=\"tg-5rcs\" colspan=\"3\">Inputs</th>\n",
    "    <th class=\"tg-5rcs\">Output</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">1</td>\n",
    "    <td class=\"tg-4kyz\">0</td>\n",
    "  </tr>\n",
    "</tbody></table>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output simply depends on whether the first input is a 1 or not. The 2nd input is irrelevant and the 3rd input is our bias (explained elsewhere).\n",
    "<p>If you train the 2-layer neural net (thus one set of weights) using gradient descent using the implementation in the iamtrask article, you will get a set of weights close to this:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [[ 9.67299303],[-0.2078435],[-4.62963669]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you calculate the cost using these weights (the cost function is a simple difference between expected and actual output values), you get..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cost: 0.0557587344696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty low right? Now just to jump ahead a bit, when I tuned the genetic algorithm and ran it a couple of times, it found a completely different set of weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [[  3.09686945e+05  -7.88485054e-03  -1.67477116e+03]]\n",
    "> <br />Cost: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously these weights resulted in a significantly lower cost (better fitness). In all honesty however, for this simplistic problem, the difference in cost is pretty inconsequential. In more complex problems, a cost that low is probably resulting in overfitting. Not to mention, genetic algorithms almost certainly will take longer to converge than gradient descent. But let's ignore all those details, we just want to build a genetic algorithm because they're cool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I jump into the details of the genetic algorithm, I want to revisit the neural net. Here's the code for the neural net I implemented, which is an adaptation from the one by iamtrask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1]]) #training data X\n",
    "y = np.array([[0,0,1,1, 1, 0, 0]]).T #training data Y\n",
    "syn0 = 2*np.random.random((3,1)) - 1 #randomize intial weights (Theta)\n",
    "\n",
    "def runForward(X, theta): #this runs our net and returns the output\n",
    "\treturn sigmoid(np.dot(X, theta))\n",
    "def costFunction(X, y, theta): #our cost function, simply determines the arithmetic difference between the expected y and our actual y\n",
    "\tm = float(len(X))\n",
    "\thThetaX = np.array(runForward(X, theta))\n",
    "\treturn np.sum(np.abs(y - hThetaX))\n",
    "def sigmoid(x): return 1 / (1 + np.exp(- x)) #Just our run-of-the-mill sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to figure this out if you've run through the iamtrask article or already have an understanding of neural nets. Just a note: I call the weights Theta. Let's go ahead and run this network just to make sure it's working right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31364485])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runForward(np.array([0,1,1]), syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect to get about [ 0 ] for an input of [0,1,1], but obviously when we use random weights, that's not likely to happen. Let's try again with those weights I got from doing gradient descent (not shown here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00786466])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_theta = np.array([[ 9.67299303],[-0.2078435],[-4.62963669]])\n",
    "runForward(np.array([0,1,1]), optimal_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get a value pretty close to 0, as expected. Nice. Okay, so now let's try the weights I got from running the genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/kernel/__main__.py:14: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_theta_ga = np.array([3.09686945e+05,-7.88485054e-03,-1.67477116e+03])\n",
    "runForward(np.array([0,1,1]), optimal_theta_ga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The result we get here is so close to zero we get an overflow warning. Just ignore that, the point is, the error/cost is really, really low. (Again, this is not necessarily a good thing...becauase of potential overfitting, but for this particular problem, overfitting is fine). Just to make things really clear, let's take a look at what these weights are doing diagramatically.\n",
    "<div>\n",
    "<img src=\"images/NNDiagram1.png\" width=\"200px\" style=\"display:inline-block;\" /><img style=\"display:inline-block;\"  src=\"images/NNDiagram2.png\" width=\"200px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So as you can see on the right, whenever the bottom (left-most) input is 1, a really, really big number gets sent over to our sigmoid function, which will of course return something very close to 1.\n",
    "<p>Alright, so enough about the neural network. Let's go line by line with the genetic algorithm (skipping imports).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [100, 0.05, 250, 3, 20] #These are just some parameters for the GA, defined below in order:\n",
    "# [Init pop (pop=100), mut rate (=5%), num generations (250), chromosome/solution length (3), # winners/per gen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too interesting there, but just noting that params[3] (solution length) refers to the number of elements in each individual solution. Since our solutions are weights to the 2-layer neural net, each solution is a 3 element vector (numpy array). Also need to note the last parameter, params[4] refers to how many solutions we will pick as winners from each generation to populate the new generation. So out of total population of 100, every generation we will preferentially pick the top 20 solutions, populate the new generation with them and their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curPop = np.random.choice(np.arange(-15,15,step=0.01),size=(params[0],params[3]),replace=False)\n",
    "nextPop = np.zeros((curPop.shape[0], curPop.shape[1]))\n",
    "fitVec = np.zeros((params[0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top line is the most important here. Basically we're creating a matrix 100x3 with an initial population of random solutions. We're using the np.arange() function to create a bunch of values -15, -14.99, -14.89....15  in order in a long array, then we use np.random.choice() to randomly choose 100x3 = 300 of them to build the 100x3 matrix of initial solutions. This isn't the most computationally efficient way to do things, but I've found it works really well. This is certainly not the only way to do it, and I encourage you to mess around with different ways to intialize your population. It turns out this step is really important to how well it does. If your initial population is not well randomized and not very diverse, you won't get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(params[2]):\n",
    "\tfitVec = np.array([np.array([x, np.sum(NN.costFunction(NN.X, NN.y, curPop[x].reshape(3,1)))]) for x in range(params[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params[2] is our number of generations, so this is our main, outer loop to go through the whole flow each generation.\n",
    "Our first step is to calculate the cost/error of each solution (there's 100) and add it to a matrix called <b>fitVec</b>. Each element of fitVec is an array consisting of the index of the solution in curPop and its cost, e.g. [0, 2.54] means that the 0th element in curPop (first solution) has an error of 2.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "winners = np.zeros((params[4], params[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a new matrix called <b>winners</b>; this will hold our winning solutions temporarily until we move them to the next generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tfor n in range(len(winners)):\n",
    "\t\tselected = np.random.choice(range(len(fitVec)), params[4]/2, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in a loop to populate the winners matrix. We use np.random.choice() to randomly pick params[4]/2 (20/2=10) solutions. We're gonna use a <b>tournament style selection</b> process where we randomly choose a subset of our population, and then pick the best solution from that subset and add it to our winners array. Obviously higher fitness (lower error) solutions have a higher chance of making it to the winners array, but we don't just pick the top 20 solutions because we want to maintain some genetic diversity in each generation, so have a few higher error solutions is generally a good thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        wnr = np.argmin(fitVec[selected,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the array 'selected' contains 10 random solutions (actually the indices to 10 solutions) from our population. Now we reference fitVec to find the actual elements, use np.argmin() to pick the one with the smallest error/cost and assign the index of that winning element to a variable, 'wnr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        winners[n] = curPop[int(fitVec[selected[wnr]][0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we reference the winner in curPop, the array of all solutions of the current generation, and copy it to our 'winners' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tnextPop[:len(winners)] = winners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nextPop is the array containing all the solutions for the next generation. We populate the first 20 elements of nextPop with our winning solutions from 'winners' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tnextPop[len(winners):] = np.array([np.array(np.random.permutation(np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0))) for x in range(winners.shape[1])]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, yeah this is a really long line and it's not very readable. I kind of cheated to make this all in 15 lines.\n",
    "This line is our <b>mating</b> process, and it's probably the most complicated part of a genetic algorithm. Let's start with the core of this nasty line. <br />\n",
    "\n",
    "> `np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0)` <br />\n",
    "\n",
    "Basically np.repeat() will duplicate our 20x3 matrix to create a 80x3 matrix. We already populated the first 20 elements of nextPop with the winners from last generation. Now we want to populate the last 80 elements with their offspring.\n",
    "\n",
    "> `np.random.permutation(np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0))`\n",
    "\n",
    "Now we just use np.random.permutation() to shuffle the columns of this next 80x3 matrix. This is how we accomplish the crossover functional. Imagine we have a 3x3 matrix (2 solutions) like this:<br /> `np.array([[1,2,3],[4,5,6],[7,8,9]])` , when we run the permutation function, it will change it something like:<br /> `np.array([[7,5,3],[1,8,9],[4,2,3]])`\n",
    "<br />Go look at the numpy documentation to learn more about permutation if you still don't understand how it's working here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tcurPop = np.multiply(nextPop, np.matrix([np.float(np.random.normal(0,2,1)) if random.random() < params[1] else 1 for x in range(nextPop.size)]).reshape(nextPop.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh. Our last line of code! This is our <b>mutation</b> process. I'm using a list comprehension to build a matrix of the same dimensions as nextPop, but filled with 1s. However, with a probability of params[1] (our mutation rate), we randomly \"mutate\" some of the elements. Our mutation is basically using a random value from numpy.random.normal() instead of 1. So we end up with a matrix like this (I've shrunk it to 10x3 to make it fit here and changed the mutation rate to 20% so you can see more mutated elements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.        ,  1.        , -0.274611  ],\n",
       "        [ 1.        ,  1.        ,  1.        ],\n",
       "        [ 1.        ,  1.        ,  1.        ],\n",
       "        [ 1.        ,  0.05055137,  1.98058061],\n",
       "        [ 1.        ,  2.62563321,  1.        ],\n",
       "        [ 1.        ,  1.        ,  3.35454534],\n",
       "        [ 1.        ,  1.        ,  1.        ],\n",
       "        [ 1.        ,  1.        , -1.96160288],\n",
       "        [ 1.        ,  1.        ,  1.0381105 ],\n",
       "        [-0.64245344,  2.04278346,  1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix([np.float(np.random.normal(0,2,1)) if random.random() < 0.20 else 1 for x in range(30)]).reshape(10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply this matrix (element-wise multiplication) to our nextPop matrix. Most of the time we're multiplying each element in nextPop by 1, so leaving them unchanged, but sometimes we multiply by one of the mutated values and thus will randomly change some elements in nextPop. This adds genetic diversity to our next generation. So now we've filled up nextPop with a new generation of higher fitness solutions. We just repeat this process for how ever many generations we defined in params.\n",
    "<p><h4>Alright, so we're done! That's it! We made a genetic algorithm that trains a neural network, cool!</h4></p>\n",
    "<p>...Okay, yeah technically we did, but let's actually watch it do something. Here we go...</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Gen: #0) Total error: 360.621647726\n",
      "\n",
      "(Gen: #1) Total error: 164.147038902\n",
      "\n",
      "(Gen: #2) Total error: 42.1044097511\n",
      "\n",
      "(Gen: #3) Total error: 24.2327815785\n",
      "\n",
      "(Gen: #4) Total error: 17.0591462061\n",
      "\n",
      "(Gen: #5) Total error: 11.0531107277\n",
      "\n",
      "(Gen: #6) Total error: 14.4015531463\n",
      "\n",
      "(Gen: #7) Total error: 21.6185558409\n",
      "\n",
      "(Gen: #8) Total error: 12.6716347732\n",
      "\n",
      "(Gen: #9) Total error: 13.5456215099\n",
      "\n",
      "Best Sol'n:\n",
      "[[ 139.26240253    2.64168216   -8.3325537 ]]\n",
      "Cost:0.00721156902032\n",
      "When X = \n",
      "[[0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]] \n",
      "hThetaX = \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "import NeuralNet as NN\n",
    "params = [100, 0.05, 10, 3, 20] # [Init pop (pop=100), mut rate (=5%), num generations (250), chromosome/solution length (3), # winners/per gen]\n",
    "curPop = np.random.choice(np.arange(-15,15,step=0.01),size=(params[0],params[3]),replace=False) #initialize current population to random values within range\n",
    "nextPop = np.zeros((curPop.shape[0], curPop.shape[1]))\n",
    "fitVec = np.zeros((params[0], 2)) #1st col is indices, 2nd col is cost\n",
    "for i in range(params[2]): #iterate through num generations\n",
    "\tfitVec = np.array([np.array([x, np.sum(NN.costFunction(NN.X, NN.y, curPop[x].reshape(3,1)))]) for x in range(params[0])]) #Create vec of all errors from cost function\n",
    "\tprint(\"(Gen: #%s) Total error: %s\\n\" % (i, np.sum(fitVec[:,1])))\n",
    "\twinners = np.zeros((params[4], params[3])) #20x2\n",
    "\tfor n in range(len(winners)): #for n in range(10)\n",
    "\t\tselected = np.random.choice(range(len(fitVec)), params[4]/2, replace=False)\n",
    "\t\twnr = np.argmin(fitVec[selected,1])\n",
    "\t\twinners[n] = curPop[int(fitVec[selected[wnr]][0])]\n",
    "\tnextPop[:len(winners)] = winners #populate new gen with winners\n",
    "\tnextPop[len(winners):] = np.array([np.array(np.random.permutation(np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0))) for x in range(winners.shape[1])]).T #Populate the rest of the generation with offspring of mating pairs\n",
    "\tnextPop = np.multiply(nextPop, np.matrix([np.float(np.random.normal(0,2,1)) if random.random() < params[1] else 1 for x in range(nextPop.size)]).reshape(nextPop.shape)) #randomly mutate part of the population\n",
    "\tcurPop = nextPop\n",
    "\n",
    "best_soln = curPop[np.argmin(fitVec[:,1])]\n",
    "X = np.array([[0,1,1],[1,1,1],[0,0,1],[1,0,1]])\n",
    "result = np.round(NN.runForward(X, best_soln.reshape(3,1)))\n",
    "print(\"Best Sol'n:\\n%s\\nCost:%s\" % (best_soln,np.sum(NN.costFunction(NN.X, NN.y, best_soln.reshape(3,1)))))\n",
    "print(\"When X = \\n%s \\nhThetaX = \\n%s\" % (X[:,:2], result,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sweet!</h3>\n",
    "Looks like it converged, finding a solution with a cost of only 0.007, pretty close to the error of the solution found with gradient descent. Notice I only ran it for 10 generations 1) because clearly that's all it takes and 2) because I didn't want a 20 page long document here.\n",
    "\n",
    "<p><h2>Where to go from here?</h2>\n",
    "First off, thanks. If you made it this far, I must've done something right. <br />\n",
    "But if you want to do more, then I really encourage you to play around with the parameters, maybe change up the neural network, or change the neural network cost function, etc and see what happens. The best way to learn is to get your hands dirty. Keep in mind this GA was kind of hard-wired for our little neural net by iamtrask, but if you understand the concepts and methods, you should be able to adapt it to more complex problems.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bonus! Let's graph the population errors vs the generation #</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Sol'n:\n",
      "[[ 625.07750262  -48.12579247 -532.48348958]]\n",
      "Cost:1.11272735801e-231\n",
      "When X = \n",
      "[[0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 0]] \n",
      "hThetaX = \n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandonbrown/Desktop/Projects/MiniGAandNN/NeuralNet.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x): return 1 / (1 + np.exp(- x))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLWfQS1LL/X8e/85GQ5DVkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106ed5588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "import NeuralNet as NN\n",
    "params = [100, 0.05, 25, 3, 20] # [Init pop (pop=100), mut rate (=5%), num generations (250), chromosome/solution length (3), # winners/per gen]\n",
    "curPop = np.random.choice(np.arange(-15,15,step=0.01),size=(params[0],params[3]),replace=False) #initialize current population to random values within range\n",
    "nextPop = np.zeros((curPop.shape[0], curPop.shape[1]))\n",
    "fitVec = np.zeros((params[0], 2)) #1st col is indices, 2nd col is cost\n",
    "for i in range(params[2]): #iterate through num generations\n",
    "\tfitVec = np.array([np.array([x, np.sum(NN.costFunction(NN.X, NN.y, curPop[x].reshape(3,1)))]) for x in range(params[0])]) #Create vec of all errors from cost function\n",
    "\tplt.pyplot.scatter(i,np.sum(fitVec[:,1]))\n",
    "\twinners = np.zeros((params[4], params[3])) #20x2\n",
    "\tfor n in range(len(winners)): #for n in range(10)\n",
    "\t\tselected = np.random.choice(range(len(fitVec)), params[4]/2, replace=False)\n",
    "\t\twnr = np.argmin(fitVec[selected,1])\n",
    "\t\twinners[n] = curPop[int(fitVec[selected[wnr]][0])]\n",
    "\tnextPop[:len(winners)] = winners #populate new gen with winners\n",
    "\tnextPop[len(winners):] = np.array([np.array(np.random.permutation(np.repeat(winners[:, x], ((params[0] - len(winners))/len(winners)), axis=0))) for x in range(winners.shape[1])]).T #Populate the rest of the generation with offspring of mating pairs\n",
    "\tnextPop = np.multiply(nextPop, np.matrix([np.float(np.random.normal(0,2,1)) if random.random() < params[1] else 1 for x in range(nextPop.size)]).reshape(nextPop.shape)) #randomly mutate part of the population\n",
    "\tcurPop = nextPop\n",
    "\n",
    "best_soln = curPop[np.argmin(fitVec[:,1])]\n",
    "X = np.array([[0,1,1],[1,1,1],[0,0,1],[1,0,1]])\n",
    "result = np.round(NN.runForward(X, best_soln.reshape(3,1)))\n",
    "print(\"Best Sol'n:\\n%s\\nCost:%s\" % (best_soln,np.sum(NN.costFunction(NN.X, NN.y, best_soln.reshape(3,1)))))\n",
    "print(\"When X = \\n%s \\nhThetaX = \\n%s\" % (X[:,:2], result,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we converge after just <b>4</b> generations! Also notice we get these little bumps in error every 10 generations or so, likely due to a particularly dramatic mutation round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
